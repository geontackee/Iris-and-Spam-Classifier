{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "936368cf-98e5-4578-af3d-8508af6f1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05965b-a2b2-4378-952c-3e61a672b428",
   "metadata": {},
   "source": [
    "## Load Dataset (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a36e5af-2626-41df-a63d-97a365cd75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(\"iris.csv\", names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "with pd.option_context('future.no_silent_downcasting', True):\n",
    "    iris_df= iris_df.replace({'Iris-setosa':0, 'Iris-versicolor': 1, 'Iris-virginica': 2}).infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64cc209e-e15d-41bb-9d80-a9c3cad63c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  class\n",
       "0             5.1          3.5           1.4          0.2      0\n",
       "1             4.9          3.0           1.4          0.2      0\n",
       "2             4.7          3.2           1.3          0.2      0\n",
       "3             4.6          3.1           1.5          0.2      0\n",
       "4             5.0          3.6           1.4          0.2      0\n",
       "..            ...          ...           ...          ...    ...\n",
       "145           6.7          3.0           5.2          2.3      2\n",
       "146           6.3          2.5           5.0          1.9      2\n",
       "147           6.5          3.0           5.2          2.0      2\n",
       "148           6.2          3.4           5.4          2.3      2\n",
       "149           5.9          3.0           5.1          1.8      2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa32b4d-ab99-4d07-8fba-efe20e01d64a",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4c097f-f87e-4fd7-b928-2574e561ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "target_col = 'class'\n",
    "\n",
    "X = np.array(iris_df[feature_cols].values)\n",
    "y = np.array(iris_df[target_col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d0ca74-cbf7-4c5d-a712-ce3aacc63251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Kfolds\n",
    "kfolds = KFold(n_splits=10, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e97a73-314e-45cd-9bdf-a4b9395bef32",
   "metadata": {},
   "source": [
    "## Entropy, Information Gain, Node Class, and DecisionTree Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8cd68c-a4db-475e-97b1-398053b7f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    count = np.bincount(y) #Counting how many of each class occurs\n",
    "    probs = count / len(y) #Calculating the probabilities of each respective class\n",
    "    return -sum(p * np.log2(p) for p in probs if p > 0) #Entropy calculation. When log2(0), it is infinity, but p is 0, resulting in 0. hence, with the if statement, skip to avoid errors.\n",
    "    \n",
    "def information_gain(X_column, y, threshold):\n",
    "    parent_entropy = entropy(y) #Calculating parent entropy\n",
    "    l_mask, r_mask = X_column <= threshold, X_column > threshold #Creating a boolean array to classify elements to left or right subtree. \n",
    "    l_child, r_child = y[l_mask], y[r_mask] #Classifying left and right child using boolean array retrieved from previous line\n",
    "    l_entropy, r_entropy = entropy(l_child), entropy(r_child) #Calculating the respective entropies of left and right child\n",
    "    weighted_child_entropy = (len(l_child)/len(y)) * l_entropy + (len(r_child)/len(y)) * r_entropy #Calculating weighted entropy. Weight is the proportion of the subtree against all.\n",
    "\n",
    "    return parent_entropy - weighted_child_entropy #Calculating then returning information gain.\n",
    "\n",
    "class Node: #Node class for decision tree implementation\n",
    "    def __init__(self, feature_index = None, threshold = None, left = None, right = None, value = None ):\n",
    "        self.feature_index = feature_index #To denote which feature was used for threshold\n",
    "        self.threshold = threshold #Threshold of the split\n",
    "        self.left = left #Left subtree\n",
    "        self.right = right #Right subtree\n",
    "        self.value = value #Value. When value = None, it is one of the inner nodes. If value equals to some class, it is a leaf node.\n",
    "        \n",
    "class DecisionTree: #Decision tree class\n",
    "    def __init__(self, root=None, nmin = None):\n",
    "        self.root = root #Setting root of decision tree\n",
    "        self.nmin = nmin #Declaring Nmin for the decision tree\n",
    "\n",
    "    def best_threshold(self, X, y, num_features): #Calculating the best threshold\n",
    "        best_gain = -float('inf')\n",
    "        best_threshold = None\n",
    "        best_feature = None\n",
    "        \n",
    "        for feature_index in range(num_features): #Itearting through all possible features\n",
    "            thresholds = np.unique(X[:,feature_index]) #Sorting and removing duplicate values. Sorting is necessary for midpoint calculation.\n",
    "            thresholds = (thresholds[1:] + thresholds[:-1])/2 #Calculating all possible midpoints.\n",
    "            \n",
    "            for threshold in thresholds:#Iterating through all possible thresholds with respect to the current feature value.\n",
    "                gain = information_gain(X[:,feature_index], y, threshold) #Calculating the information gain with respect to the feautre and its threshold.\n",
    "\n",
    "                if gain > best_gain: #If current gain is greater than the overall information gain, meaning its the best threshold.\n",
    "                    best_gain = gain #Updating best information gain\n",
    "                    best_threshold = threshold #Updating best threshold\n",
    "                    best_feature = feature_index #Updating which feature the threshold is applied to.\n",
    "\n",
    "        return best_feature, best_threshold #Return the best feature with the best threshold.\n",
    "\n",
    "    def fit_tree(self, X, y): #Training the decision tree.\n",
    "        if len(y) < self.nmin or len(set(y)) == 1: #Base case: split n < nmin or when there are only one type of class\n",
    "            values, counts = np.unique(y, return_counts = True)\n",
    "            return Node(value = values[np.argmax(counts)]) #Calculating which class is dominant, and setting this leaf node to that class.\n",
    "\n",
    "        best_feature, best_threshold = self.best_threshold(X,y, len(X[0])) #Calculating the best feature and threshold from all possible feature and thresholds.\n",
    "\n",
    "        if best_threshold is None or best_feature is None: #If there are no further splits available, it is a leaf node.\n",
    "            values, counts = np.unique(y, return_counts = True)\n",
    "            return Node(value = values[np.argmax(counts)]) #Calculating which class is dominant, and setting this leaf node to that class.\n",
    "\n",
    "        left_mask = X[:, best_feature] <= best_threshold #Dividing data to left and right child, based on the boolean array calculated.\n",
    "        right_mask = X[:, best_feature] > best_threshold\n",
    "\n",
    "        l_child, r_child = y[left_mask], y[right_mask]\n",
    "\n",
    "        if len(l_child) < self.nmin or len(r_child) < self.nmin: #If the split creates a child with n < self.nmin, stop splitting\n",
    "            values, counts = np.unique(y, return_counts = True)\n",
    "            return Node(value = values[np.argmax(counts)]) #Calculating which class is dominant, and setting this leaf node to that class.\n",
    "        \n",
    "        if len(l_child) == 0 or len(r_child) == 0: #If either left or right child is empty, it is a leaf node.\n",
    "            values, counts = np.unique(y, return_counts = True)\n",
    "            return Node(value = values[np.argmax(counts)]) #Calculating which class is dominant, and setting this leaf node to that class.\n",
    "            \n",
    "        l_subtree = self.fit_tree(X[left_mask], y[left_mask]) #Recursively creating left and right subtree.\n",
    "        r_subtree = self.fit_tree(X[right_mask], y[right_mask])\n",
    "\n",
    "        return Node(feature_index = best_feature, threshold = best_threshold, left = l_subtree, right = r_subtree) #Returning the root of the decision tree.\n",
    "\n",
    "    def predict(self, x, node): #Predicting sample by sample by iterating through the decision tree and finding which leaf node it belongs to. Return the class of the leaf node.\n",
    "        if node.value is not None: #Base case: leaf node reached. Return the node value.\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold: #Move to left subtree\n",
    "            return self.predict(x, node.left)\n",
    "        else:\n",
    "            return self.predict(x, node.right) #Move to right subtree\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a76d3b5-da33-49d4-b0a7-9a92c13bb561",
   "metadata": {},
   "source": [
    "## Training and Testing Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff97c081-0db2-4811-b3f7-ad5f8cd69d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for nmin = 5: [1.0, 1.0, 1.0, 0.8, 1.0, 0.9333333333333333, 0.8666666666666667, 0.8666666666666667, 1.0, 0.8666666666666667]\n",
      "Accuracy scores for nmin = 10: [1.0, 0.8, 1.0, 1.0, 0.8666666666666667, 0.9333333333333333, 1.0, 0.9333333333333333, 1.0, 0.9333333333333333]\n",
      "Accuracy scores for nmin = 15: [1.0, 1.0, 0.9333333333333333, 0.8666666666666667, 1.0, 1.0, 0.8666666666666667, 0.9333333333333333, 0.9333333333333333, 0.9333333333333333]\n",
      "Accuracy scores for nmin = 20: [0.9333333333333333, 0.9333333333333333, 1.0, 1.0, 0.8666666666666667, 0.9333333333333333, 1.0, 0.9333333333333333, 1.0, 0.9333333333333333]\n"
     ]
    }
   ],
   "source": [
    "nmins = [5, 10, 15, 20]\n",
    "accuracy_scores_per_nmin = []\n",
    "\n",
    "for nmin in nmins: #Training and testing Iris dataset on different nmins.\n",
    "    accuracy_scores = [] #Scoring the acccuracies across all kfolds.\n",
    "\n",
    "    for train_idx, test_idx in kfolds.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "        iris_model = DecisionTree(nmin = nmin) #Creating a decision tree of nmin = nmin.\n",
    "        iris_model.root = iris_model.fit_tree(X_train, y_train) #Fitting tree.\n",
    "        y_pred = []\n",
    "        for x in X_test: #Predicting element by element\n",
    "            y_pred.append(iris_model.predict(x, iris_model.root)) #Storing the prediction on the y_pred array.\n",
    "        accuracy = accuracy_score(y_test, y_pred) #Calculating accuracy.\n",
    "        accuracy_scores.append(accuracy) #Storing accuracy of each fold in accuracy_scores\n",
    "        \n",
    "    print(f\"Accuracy scores for nmin = {nmin}: {accuracy_scores}\")\n",
    "    accuracy_scores_per_nmin.append(accuracy_scores) #Storing the accuracy of the nmin across all folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216789a9-8e4c-42fd-927c-7b9dcd4b0e53",
   "metadata": {},
   "source": [
    "## Results of Iris Dataset Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c3b368-6c4e-4f93-9552-131bf0b1bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Nmin  Accuracy  Variance\n",
      "0     5  0.933333  0.005333\n",
      "1    10  0.946667  0.004267\n",
      "2    15  0.946667  0.002489\n",
      "3    20  0.953333  0.001822\n"
     ]
    }
   ],
   "source": [
    "iris_average_accuracy = [np.mean(acc) for acc in accuracy_scores_per_nmin] #Calculating average accuracy for each nmin across the folds\n",
    "iris_variance = [np.var(acc) for acc in accuracy_scores_per_nmin] #Calculating variance for each nmin across the folds\n",
    "\n",
    "iris_model_data = { #Creating a pandas dataframe to better visualize as a table\n",
    "    \"Nmin\" : nmins,\n",
    "    \"Accuracy\" : iris_average_accuracy,\n",
    "    \"Variance\" : iris_variance\n",
    "}\n",
    "\n",
    "iris_model_df = pd.DataFrame(iris_model_data)\n",
    "\n",
    "print(iris_model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6eda1c-5df3-47c5-8d4a-af79d66eb4f9",
   "metadata": {},
   "source": [
    "Nmin's effect can be clearly seen in models regarding the iris dataset. Since the sample size is small (150), the affect of nmin is large. When nmin is small, the decision tree will go deep, overfitting with high variance. When nim is large, the tree will be shallow, underfitting with low variance. This trend can clearly be seen as nmin grows from 5 to 20, where nimin = 5 has the lowerst accuracy and highest variance, while nmin = 20 has the highest accuracy with the lowest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6253ff-d8ea-4497-a3b8-061500900457",
   "metadata": {},
   "source": [
    "## Load Dataset (Spambase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817bfd2d-12d9-4bc6-8aee-bbca0cfc9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spambase_df = pd.read_csv(\"spambase.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d4a688f-c74e-4b77-9943-7e3ba343f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4600 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...   0.41  \\\n",
       "0     0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "1     0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "2     0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "3     0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.000   \n",
       "...    ...   ...     ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4595  0.31  0.00    0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4596  0.00  0.00    0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.30  0.00    0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4598  0.96  0.00    0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4599  0.00  0.00    0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "       0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0     0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1     0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2     0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3     0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4     0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "...     ...   ...    ...    ...    ...    ...  ...   ... ..  \n",
       "4595  0.232   0.0  0.000  0.000  0.000  1.142    3    88  0  \n",
       "4596  0.000   0.0  0.353  0.000  0.000  1.555    4    14  0  \n",
       "4597  0.718   0.0  0.000  0.000  0.000  1.404    6   118  0  \n",
       "4598  0.057   0.0  0.000  0.000  0.000  1.147    5    78  0  \n",
       "4599  0.000   0.0  0.125  0.000  0.000  1.250    5    40  0  \n",
       "\n",
       "[4600 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91247a9-cbb9-4d99-ad82-e1788abc987d",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159bff0b-6561-42bb-bf15-ca6538a8b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spam = np.array(spambase_df.iloc[:, :-1])\n",
    "y_spam = np.array(spambase_df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35e286-dd21-4a94-a213-26360dd5c934",
   "metadata": {},
   "source": [
    "## Training and Testing Spambase Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a4ffcdf-c070-4c8a-9c5d-2190344e0f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores for nmin = 5: [0.9195652173913044, 0.9043478260869565, 0.9369565217391305, 0.9282608695652174, 0.9152173913043479, 0.9434782608695652, 0.9282608695652174, 0.9304347826086956, 0.9217391304347826, 0.8913043478260869]\n",
      "Accuracy scores for nmin = 10: [0.9152173913043479, 0.9152173913043479, 0.908695652173913, 0.9130434782608695, 0.9282608695652174, 0.9478260869565217, 0.9260869565217391, 0.8934782608695652, 0.9043478260869565, 0.9152173913043479]\n",
      "Accuracy scores for nmin = 15: [0.9239130434782609, 0.9021739130434783, 0.9021739130434783, 0.9108695652173913, 0.9065217391304348, 0.8891304347826087, 0.9282608695652174, 0.9391304347826087, 0.8934782608695652, 0.9282608695652174]\n",
      "Accuracy scores for nmin = 20: [0.9369565217391305, 0.9108695652173913, 0.8956521739130435, 0.9130434782608695, 0.9282608695652174, 0.8913043478260869, 0.908695652173913, 0.8869565217391304, 0.9, 0.8804347826086957]\n",
      "Accuracy scores for nmin = 25: [0.9347826086956522, 0.8956521739130435, 0.8934782608695652, 0.8934782608695652, 0.9065217391304348, 0.9195652173913044, 0.9152173913043479, 0.8913043478260869, 0.9304347826086956, 0.8978260869565218]\n"
     ]
    }
   ],
   "source": [
    "spambase_nmins = [5, 10, 15, 20, 25]\n",
    "spambase_accuracy_scores_per_nmin = []\n",
    "\n",
    "for nmin in spambase_nmins: #Training and testing Iris dataset on different nmins.\n",
    "    accuracy_scores = [] #Scoring the acccuracies across all kfolds.\n",
    "\n",
    "    for train_idx, test_idx in kfolds.split(X_spam):\n",
    "        X_train, X_test = X_spam[train_idx], X_spam[test_idx]\n",
    "        y_train, y_test = y_spam[train_idx], y_spam[test_idx]\n",
    "    \n",
    "        spambase_model = DecisionTree(nmin = nmin) #Creating a decision tree of nmin = nmin.\n",
    "        spambase_model.root = spambase_model.fit_tree(X_train, y_train) #Fitting tree.\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            y_pred.append(iris_model.predict(x, spambase_model.root)) #Storing the prediction on the y_pred array.\n",
    "        accuracy = accuracy_score(y_test, y_pred) #Calculating accuracy.\n",
    "        accuracy_scores.append(accuracy) #Storing accuracy of each fold in accuracy_scores\n",
    "        \n",
    "    print(f\"Accuracy scores for nmin = {nmin}: {accuracy_scores}\")\n",
    "    spambase_accuracy_scores_per_nmin.append(accuracy_scores) #Storing the accuracy of the nmin across all folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae85e28-9bfd-4cd9-b0c2-1b03f202474d",
   "metadata": {},
   "source": [
    "## Results of Spambase Dataset Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e13d38-aa88-4d8e-8e77-0488c0e45aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Nmin  Accuracy  Variance\n",
      "0     5  0.921957  0.000214\n",
      "1    10  0.916739  0.000197\n",
      "2    15  0.912391  0.000250\n",
      "3    20  0.905217  0.000290\n",
      "4    25  0.907826  0.000236\n"
     ]
    }
   ],
   "source": [
    "spambase_average_accuracy = [np.mean(acc) for acc in spambase_accuracy_scores_per_nmin] #Calculating average accuracy for each nmin across the folds\n",
    "spambase_variance = [np.var(acc) for acc in spambase_accuracy_scores_per_nmin] #Calculating variance for each nmin across the folds\n",
    "\n",
    "spambase_model_data = { #Creating a pandas dataframe to better visualize as a table\n",
    "    \"Nmin\" : spambase_nmins,\n",
    "    \"Accuracy\" : spambase_average_accuracy,\n",
    "    \"Variance\" : spambase_variance\n",
    "}\n",
    "\n",
    "spambase_model_df = pd.DataFrame(spambase_model_data)\n",
    "\n",
    "print(spambase_model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea35289-ba74-44b8-9e99-dd3e82f24dc5",
   "metadata": {},
   "source": [
    "Nmin's effect is not clearly seen in the spambase model. Since the dataset is large (4600 samples), the optimum nmin seems to be larger than 25, as we could see the accuracy went slightly up and variance went slightly down from nmin = 20 to nmin = 25. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
